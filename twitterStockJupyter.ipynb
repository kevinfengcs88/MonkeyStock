{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['polarity','id','date','flag','user','text']\n",
    "set_encoding = \"ISO-8859-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding=set_encoding, names=cols)\n",
    "#df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1361722</th>\n",
       "      <td>GRADUATION TODAY!!!!! SO EXCITING</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916117</th>\n",
       "      <td>fun night tonight, and my house looks so purrr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303571</th>\n",
       "      <td>Bubbles died.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531916</th>\n",
       "      <td>And now I have a phone call and can't listen t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182906</th>\n",
       "      <td>@flourishingjudy  - now we just need to get #t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147008</th>\n",
       "      <td>im okay with the fact that im a lesbian at the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521917</th>\n",
       "      <td>@breathe_in that'd be nice</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463769</th>\n",
       "      <td>@Kell_Krushka i know, it makes me sad too.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784773</th>\n",
       "      <td>im really tired... no sleep last nite</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172508</th>\n",
       "      <td>New Funnelize Firefox themes installed on http...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  polarity\n",
       "1361722                 GRADUATION TODAY!!!!! SO EXCITING          4\n",
       "916117   fun night tonight, and my house looks so purrr...         4\n",
       "303571                                      Bubbles died.          0\n",
       "531916   And now I have a phone call and can't listen t...         0\n",
       "1182906  @flourishingjudy  - now we just need to get #t...         4\n",
       "1147008  im okay with the fact that im a lesbian at the...         4\n",
       "1521917                        @breathe_in that'd be nice          4\n",
       "463769        @Kell_Krushka i know, it makes me sad too.           0\n",
       "784773              im really tired... no sleep last nite          0\n",
       "1172508  New Funnelize Firefox themes installed on http...         4"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df[['text','polarity']]\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/vkwnpm091dqdlm0xqwfjxq_80000gn/T/ipykernel_56187/335142359.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['polarity'] = data['polarity'].replace(4,1)\n"
     ]
    }
   ],
   "source": [
    "data['polarity'] = data['polarity'].replace(4,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19995    not much time off this weekend, work trip to m...\n",
       "19996                            one more day of holidays \n",
       "19997    feeling so down right now .. i hate you damn h...\n",
       "19998    geez,i hv to read the whole book of personalit...\n",
       "19999    i threw my sign at donnie and he bent over to ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos = data[data['polarity'] == 1].iloc[:int(20000)]\n",
    "data_neg = data[data['polarity'] == 0].iloc[:int(20000)]\n",
    "dataset = pd.concat([data_pos, data_neg])\n",
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].tail()\n",
    "#dataset.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mikebazarsky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "#print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19995    much time weekend, work trip malmï¿½ fri-sat t...\n",
       "19996                                     one day holidays\n",
       "19997                   feeling right .. hate damn humprey\n",
       "19998    geez,i hv read whole book personality types em...\n",
       "19999     threw sign donnie bent get thingee made sad face\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "print(english_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19995    much time weekend work trip malmï¿½ frisat tod...\n",
       "19996                                     one day holidays\n",
       "19997                     feeling right  hate damn humprey\n",
       "19998    geezi hv read whole book personality types emb...\n",
       "19999     threw sign donnie bent get thingee made sad face\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', english_punctuations)\n",
    "    return text.translate(translator)\n",
    "dataset['text']= dataset['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000    ['love', 'healthuandpets', 'u', 'guys', 'r', '...\n",
       "800001    ['im', 'meeting', 'one', 'besties', 'tonight',...\n",
       "800002    ['darealsunisakim', 'thanks', 'twitter', 'add'...\n",
       "800003    ['sick', 'really', 'cheap', 'hurts', 'much', '...\n",
       "800004              ['lovesbrooklyn', 'effect', 'everyone']\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing URLs\n",
    "import re\n",
    "dataset['text'] = dataset['text'].astype(str)\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_URLs(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000    ['love', 'healthuandpets', 'u', 'guys', 'r', '...\n",
       "800001    ['im', 'meeting', 'one', 'besties', 'tonight',...\n",
       "800002    ['darealsunisakim', 'thanks', 'twitter', 'add'...\n",
       "800003    ['sick', 'really', 'cheap', 'hurts', 'much', '...\n",
       "800004              ['lovesbrooklyn', 'effect', 'everyone']\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing numbers\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800002    [darealsunisakim, thanks, twitter, add, sunisa...\n",
       "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800004                    [lovesbrooklyn, effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800002    [darealsunisakim, thanks, twitter, add, sunisa...\n",
       "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800004                    [lovesbrooklyn, effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming\n",
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800002    [darealsunisakim, thanks, twitter, add, sunisa...\n",
       "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800004                    [lovesbrooklyn, effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating input feature and label\n",
    "X=data.text\n",
    "y=data.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state =26105111)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
