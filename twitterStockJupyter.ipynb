{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['polarity','id','date','flag','user','text']\n",
    "set_encoding = \"ISO-8859-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding=set_encoding, names=cols)\n",
    "#df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311015</th>\n",
       "      <td>as expected... errors</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463251</th>\n",
       "      <td>exam went gr8.... missin my friends</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386423</th>\n",
       "      <td>Pussy cat has been missing for 3 days now. Fea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045643</th>\n",
       "      <td>@cappo goodnight</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525371</th>\n",
       "      <td>@_CrC_ hells ya! But they never respond to me</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462914</th>\n",
       "      <td>Watching F1 Turkey GP at home with Beers&amp;amp;C...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207166</th>\n",
       "      <td>ok that's was freaken awsome</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25969</th>\n",
       "      <td>i want a little pet so bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437090</th>\n",
       "      <td>@trythis__ I envy you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293202</th>\n",
       "      <td>@DaivRawks  Yeah, I sure wasted a lot of time ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  polarity\n",
       "311015                              as expected... errors          0\n",
       "463251                exam went gr8.... missin my friends          0\n",
       "386423   Pussy cat has been missing for 3 days now. Fea...         0\n",
       "1045643                                  @cappo goodnight          4\n",
       "525371      @_CrC_ hells ya! But they never respond to me          0\n",
       "1462914  Watching F1 Turkey GP at home with Beers&amp;C...         4\n",
       "1207166                      ok that's was freaken awsome          4\n",
       "25969                          i want a little pet so bad          0\n",
       "437090                              @trythis__ I envy you          0\n",
       "1293202  @DaivRawks  Yeah, I sure wasted a lot of time ...         4"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df[['text','polarity']]\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/vkwnpm091dqdlm0xqwfjxq_80000gn/T/ipykernel_56187/335142359.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['polarity'] = data['polarity'].replace(4,1)\n"
     ]
    }
   ],
   "source": [
    "data['polarity'] = data['polarity'].replace(4,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19995    not much time off this weekend, work trip to m...\n",
       "19996                            one more day of holidays \n",
       "19997    feeling so down right now .. i hate you damn h...\n",
       "19998    geez,i hv to read the whole book of personalit...\n",
       "19999    i threw my sign at donnie and he bent over to ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos = data[data['polarity'] == 1].iloc[:int(20000)]\n",
    "data_neg = data[data['polarity'] == 0].iloc[:int(20000)]\n",
    "dataset = pd.concat([data_pos, data_neg])\n",
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].tail()\n",
    "#dataset.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb#ch0000010?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb#ch0000010?line=2'>3</a>\u001b[0m STOPWORDS \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "#print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19995    much time weekend, work trip malmï¿½ fri-sat t...\n",
       "19996                                     one day holidays\n",
       "19997                   feeling right .. hate damn humprey\n",
       "19998    geez,i hv read whole book personality types em...\n",
       "19999     threw sign donnie bent get thingee made sad face\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "print(english_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19995    much time weekend work trip malmï¿½ frisat tod...\n",
       "19996                                     one day holidays\n",
       "19997                     feeling right  hate damn humprey\n",
       "19998    geezi hv read whole book personality types emb...\n",
       "19999     threw sign donnie bent get thingee made sad face\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', english_punctuations)\n",
    "    return text.translate(translator)\n",
    "dataset['text']= dataset['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                   love health4uandpets u guys r best\n",
       "800001    im meeting one besties tonight cant wait  girl...\n",
       "800002    darealsunisakim thanks twitter add sunisa got ...\n",
       "800003    sick really cheap hurts much eat real food plu...\n",
       "800004                       lovesbrooklyn2 effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing URLs\n",
    "import re\n",
    "dataset['text'] = dataset['text'].astype(str)\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_URLs(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                    love healthuandpets u guys r best\n",
       "800001    im meeting one besties tonight cant wait  girl...\n",
       "800002    darealsunisakim thanks twitter add sunisa got ...\n",
       "800003    sick really cheap hurts much eat real food plu...\n",
       "800004                        lovesbrooklyn effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing numbers\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800002    [darealsunisakim, thanks, twitter, add, sunisa...\n",
       "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800004                    [lovesbrooklyn, effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800002    [darealsunisakim, thanks, twitter, add, sunisa...\n",
       "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800004                    [lovesbrooklyn, effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming\n",
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meeting, one, besties, tonight, cant, wai...\n",
       "800002    [darealsunisakim, thanks, twitter, add, sunisa...\n",
       "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
       "800004                    [lovesbrooklyn, effect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating input feature and label\n",
    "X=data.text\n",
    "y=data.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into Train and Test subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05, random_state =935)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb#ch0000038?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb#ch0000038?line=2'>3</a>\u001b[0m vectoriser \u001b[39m=\u001b[39m TfidfVectorizer(ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m), max_features\u001b[39m=\u001b[39m\u001b[39m500000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mikebazarsky/Documents/TwitterStockBot/twitterStockJupyter.ipynb#ch0000038?line=3'>4</a>\u001b[0m vectoriser\u001b[39m.\u001b[39;49mfit(X_train)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2053\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2050'>2051</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2051'>2052</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2052'>2053</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2053'>2054</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2054'>2055</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1347\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1344'>1345</a>\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1345'>1346</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1346'>1347</a>\u001b[0m X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_limit_features(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1347'>1348</a>\u001b[0m     X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1348'>1349</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1349'>1350</a>\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1350'>1351</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1163\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1160'>1161</a>\u001b[0m     mask \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m dfs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m low\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1161'>1162</a>\u001b[0m \u001b[39mif\u001b[39;00m limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m mask\u001b[39m.\u001b[39msum() \u001b[39m>\u001b[39m limit:\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1162'>1163</a>\u001b[0m     tfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(X\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\u001b[39m.\u001b[39mravel()\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1163'>1164</a>\u001b[0m     mask_inds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mtfs[mask])\u001b[39m.\u001b[39margsort()[:limit]\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1164'>1165</a>\u001b[0m     new_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(dfs), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py:624\u001b[0m, in \u001b[0;36m_cs_matrix.sum\u001b[0;34m(self, axis, dtype, out)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=619'>620</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=620'>621</a>\u001b[0m \u001b[39m# spmatrix will handle the remaining situations when axis\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=621'>622</a>\u001b[0m \u001b[39m# is in {None, -1, 0, 1}\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=622'>623</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=623'>624</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m spmatrix\u001b[39m.\u001b[39;49msum(\u001b[39mself\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py:1100\u001b[0m, in \u001b[0;36mspmatrix.sum\u001b[0;34m(self, axis, dtype, out)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1096'>1097</a>\u001b[0m \u001b[39m# axis = 0 or 1 now\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1097'>1098</a>\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1098'>1099</a>\u001b[0m     \u001b[39m# sum over columns\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1099'>1100</a>\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ascontainer(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1100'>1101</a>\u001b[0m         np\u001b[39m.\u001b[39;49mones((\u001b[39m1\u001b[39;49m, m), dtype\u001b[39m=\u001b[39;49mres_dtype)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1101'>1102</a>\u001b[0m     ) \u001b[39m@\u001b[39;49m \u001b[39mself\u001b[39;49m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1102'>1103</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1103'>1104</a>\u001b[0m     \u001b[39m# sum over rows\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1104'>1105</a>\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ascontainer(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1105'>1106</a>\u001b[0m         np\u001b[39m.\u001b[39mones((n, \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mres_dtype)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1106'>1107</a>\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py:626\u001b[0m, in \u001b[0;36mspmatrix.__rmatmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=622'>623</a>\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=623'>624</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=624'>625</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=625'>626</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rmul_dispatch(other)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py:607\u001b[0m, in \u001b[0;36mspmatrix._rmul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=604'>605</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=605'>606</a>\u001b[0m     tr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(other)\u001b[39m.\u001b[39mtranspose()\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=606'>607</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtranspose() \u001b[39m@\u001b[39;49m tr)\u001b[39m.\u001b[39mtranspose()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py:620\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=616'>617</a>\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=617'>618</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=618'>619</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=619'>620</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py:554\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=550'>551</a>\u001b[0m \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N,) \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N, \u001b[39m1\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=551'>552</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=553'>554</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_vector(np\u001b[39m.\u001b[39;49mravel(other))\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=555'>556</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, np\u001b[39m.\u001b[39mmatrix):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=556'>557</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ascontainer(result)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py:489\u001b[0m, in \u001b[0;36m_cs_matrix._mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=486'>487</a>\u001b[0m \u001b[39m# csr_matvec or csc_matvec\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=487'>488</a>\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matvec\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=488'>489</a>\u001b[0m fn(M, N, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, other, result)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=490'>491</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fit tf-dif vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform tf-dif vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80     39983\n",
      "           1       0.80      0.81      0.80     40017\n",
      "\n",
      "    accuracy                           0.80     80000\n",
      "   macro avg       0.80      0.80      0.80     80000\n",
      "weighted avg       0.80      0.80      0.80     80000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BNBmodel = BernoulliNB()\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)\n",
    "y_pred1 = BNBmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82     39983\n",
      "           1       0.81      0.82      0.82     40017\n",
      "\n",
      "    accuracy                           0.82     80000\n",
      "   macro avg       0.82      0.82      0.82     80000\n",
      "weighted avg       0.82      0.82      0.82     80000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVCmodel = LinearSVC()\n",
    "SVCmodel.fit(X_train, y_train)\n",
    "model_Evaluate(SVCmodel)\n",
    "y_pred2 = SVCmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83     39983\n",
      "           1       0.82      0.83      0.83     40017\n",
      "\n",
      "    accuracy                           0.83     80000\n",
      "   macro avg       0.83      0.83      0.83     80000\n",
      "weighted avg       0.83      0.83      0.83     80000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_Evaluate(LRmodel)\n",
    "y_pred3 = LRmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "b'[]'\n"
     ]
    }
   ],
   "source": [
    "token = 'pk_62e14354ce4f41b5bbb10e69f666afb1'\n",
    "#https://cloud.iexapis.com/stable/tops?token=YOUR_TOKEN_HERE&symbols=aapl\n",
    "\n",
    "r = requests.get('https://cloud.iexapis.com/stable/tops?token={}&symbols=aapl'.format(token))\n",
    "print(r)\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTTPS request\n",
    "r = requests.get('https://cloud.iexapis.com/stable/ref-data/iex/symbols?token={}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the names of all the stocks in the list\n",
    "stockNames = []\n",
    "for stock in r.json():\n",
    "    stockNames.append(stock['symbol'])\n",
    "#print(stockNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SMSA', 'ESBA', 'VIVE', 'SOI', 'TGR=']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def chooseStocks():\n",
    "    chosenStocks = random.sample(stockNames, 5)\n",
    "    print(chosenStocks)\n",
    "chooseStocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
